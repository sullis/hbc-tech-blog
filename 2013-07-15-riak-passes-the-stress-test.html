<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Riak Passes the Stress Test</title><title>Riak Passes the Stress Test | HBC Tech</title><meta property="og:title" content="Riak Passes the Stress Test"><meta name="author" content="HBC Tech"><meta property="og:locale" content="en_US"><meta name="description" content="In late June a few of us in NYC joined our colleagues in Gilt&rsquo;s Dublin office to test out Riak for our main user store. Every day our main user store receives upwards of 100,000 requests per minute at our peak site traffic time of 12PM. Because of these extreme traffic spikes, excellent performance of our distributed databases is a must. We also wanted to learn more about how Riak supported multiple data centers (including EC2) in an active-active-active configuration: a mechanism that guarantees continuous service in the event a database goes down. Active-active-active configurations also allow requests to be serviced in any datacenter without requiring the request to leave the data center to retrieve user information. These capabilities are quite important to Gilt, because network connections between datacenters are never as fast as those located within a single datacenter. On the hardware side, our plan was to install two rings, each consisting of five servers, in different datacenters, and then completely run Riak through the gauntlet. We had only one week to create this system. Luckily, we had the best help imaginable: Seth Thomas and Steve Vinoski from Basho, the company that created Riak. Riak passed all of our tests with high marks, and also performed well during various failure scenarios we staged. In terms of the actual testing, our goal was to duplicate our user service and point it at a Riak datastore instead of our current datastore. This, of course, required altering the service code. I&rsquo;m not a big fan of the cake pattern, which in my experience results in obfuscated code, but I have to admit that&ndash;at least in this case&ndash;it simplified the code rework and made the code cleaner. After using Riak to duplicate our user service, our next step was to write some code to copy our existing users&rsquo; information to the new Riak backend. The speed of our current infrastructure enabled us to copy our entire user set to Riak during non-peak hours. Once we had everything humming in our cluster of user services and Riak instance, we began simulating reads and writes to the service as they occurred in production. To do this, we used splitter: an open source project, created by Gilt VP Architecture Eric Bowman, that accepts requests and forwards them to two different backends. One of the responses is kept, and the other is discarded. In this case we discarded the Riak response. We then configured our load balance (Zeus) to forward requests to the splitter instead of directly to the service. This put read traffic on our test user service. To simulate write traffic, we wrote a small bit of code to publish RabbitMQ messages any time a write or update occurred; consumed those messages on the test user service; and duplicated the write or update. Read-and-writes were now being executed against the test service cluster without putting our production system in any real danger. Testing looked good so far. To really make sure Riak was as solid as it seemed, we then decided to ramp up the read/write load. We simulated an extreme read load by using Zeusbench, which executed an extreme number of HTTP requests against our test service and provided metrics on how the service performed (you can find a number of open source alternatives to Zeusbench). We also wrote a quick program that executed upwards of 1,500 insertions into Riak per second. We kept the inserts going the entire time. Read/write performance didn’t suffer, even under this amount of load. Our final test involved kill -9’ing Riak nodes and monitoring how the cluster performed. While removing Riak nodes, we maintained production load against the service. Riak performed well during these situations. Riak&rsquo;s strong performance suggests that, should we pursue implementation, it will withstand our unique traffic needs and prove reliable. As for the Gilt-Basho team’s strong performance: It was amazing that we were able to accomplish so much in just a week’s time! Thanks again to Seth and Steve for making this possible."><meta property="og:description" content="In late June a few of us in NYC joined our colleagues in Gilt&rsquo;s Dublin office to test out Riak for our main user store. Every day our main user store receives upwards of 100,000 requests per minute at our peak site traffic time of 12PM. Because of these extreme traffic spikes, excellent performance of our distributed databases is a must. We also wanted to learn more about how Riak supported multiple data centers (including EC2) in an active-active-active configuration: a mechanism that guarantees continuous service in the event a database goes down. Active-active-active configurations also allow requests to be serviced in any datacenter without requiring the request to leave the data center to retrieve user information. These capabilities are quite important to Gilt, because network connections between datacenters are never as fast as those located within a single datacenter. On the hardware side, our plan was to install two rings, each consisting of five servers, in different datacenters, and then completely run Riak through the gauntlet. We had only one week to create this system. Luckily, we had the best help imaginable: Seth Thomas and Steve Vinoski from Basho, the company that created Riak. Riak passed all of our tests with high marks, and also performed well during various failure scenarios we staged. In terms of the actual testing, our goal was to duplicate our user service and point it at a Riak datastore instead of our current datastore. This, of course, required altering the service code. I&rsquo;m not a big fan of the cake pattern, which in my experience results in obfuscated code, but I have to admit that&ndash;at least in this case&ndash;it simplified the code rework and made the code cleaner. After using Riak to duplicate our user service, our next step was to write some code to copy our existing users&rsquo; information to the new Riak backend. The speed of our current infrastructure enabled us to copy our entire user set to Riak during non-peak hours. Once we had everything humming in our cluster of user services and Riak instance, we began simulating reads and writes to the service as they occurred in production. To do this, we used splitter: an open source project, created by Gilt VP Architecture Eric Bowman, that accepts requests and forwards them to two different backends. One of the responses is kept, and the other is discarded. In this case we discarded the Riak response. We then configured our load balance (Zeus) to forward requests to the splitter instead of directly to the service. This put read traffic on our test user service. To simulate write traffic, we wrote a small bit of code to publish RabbitMQ messages any time a write or update occurred; consumed those messages on the test user service; and duplicated the write or update. Read-and-writes were now being executed against the test service cluster without putting our production system in any real danger. Testing looked good so far. To really make sure Riak was as solid as it seemed, we then decided to ramp up the read/write load. We simulated an extreme read load by using Zeusbench, which executed an extreme number of HTTP requests against our test service and provided metrics on how the service performed (you can find a number of open source alternatives to Zeusbench). We also wrote a quick program that executed upwards of 1,500 insertions into Riak per second. We kept the inserts going the entire time. Read/write performance didn’t suffer, even under this amount of load. Our final test involved kill -9’ing Riak nodes and monitoring how the cluster performed. While removing Riak nodes, we maintained production load against the service. Riak performed well during these situations. Riak&rsquo;s strong performance suggests that, should we pursue implementation, it will withstand our unique traffic needs and prove reliable. As for the Gilt-Basho team’s strong performance: It was amazing that we were able to accomplish so much in just a week’s time! Thanks again to Seth and Steve for making this possible."><link rel="canonical" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2013-07-15-riak-passes-the-stress-test.html"><meta property="og:url" content="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2013-07-15-riak-passes-the-stress-test.html"><meta property="og:site_name" content="HBC Tech"><meta property="og:type" content="article"><meta property="article:published_time" content="2013-07-15T13:41:00-05:00"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@"><meta name="twitter:creator" content="@HBC Tech"><script type="application/ld+json">{"name":null,"description":"In late June a few of us in NYC joined our colleagues in Gilt&rsquo;s Dublin office to test out Riak for our main user store. Every day our main user store receives upwards of 100,000 requests per minute at our peak site traffic time of 12PM. Because of these extreme traffic spikes, excellent performance of our distributed databases is a must. We also wanted to learn more about how Riak supported multiple data centers (including EC2) in an active-active-active configuration: a mechanism that guarantees continuous service in the event a database goes down. Active-active-active configurations also allow requests to be serviced in any datacenter without requiring the request to leave the data center to retrieve user information. These capabilities are quite important to Gilt, because network connections between datacenters are never as fast as those located within a single datacenter. On the hardware side, our plan was to install two rings, each consisting of five servers, in different datacenters, and then completely run Riak through the gauntlet. We had only one week to create this system. Luckily, we had the best help imaginable: Seth Thomas and Steve Vinoski from Basho, the company that created Riak. Riak passed all of our tests with high marks, and also performed well during various failure scenarios we staged. In terms of the actual testing, our goal was to duplicate our user service and point it at a Riak datastore instead of our current datastore. This, of course, required altering the service code. I&rsquo;m not a big fan of the cake pattern, which in my experience results in obfuscated code, but I have to admit that&ndash;at least in this case&ndash;it simplified the code rework and made the code cleaner. After using Riak to duplicate our user service, our next step was to write some code to copy our existing users&rsquo; information to the new Riak backend. The speed of our current infrastructure enabled us to copy our entire user set to Riak during non-peak hours. Once we had everything humming in our cluster of user services and Riak instance, we began simulating reads and writes to the service as they occurred in production. To do this, we used splitter: an open source project, created by Gilt VP Architecture Eric Bowman, that accepts requests and forwards them to two different backends. One of the responses is kept, and the other is discarded. In this case we discarded the Riak response. We then configured our load balance (Zeus) to forward requests to the splitter instead of directly to the service. This put read traffic on our test user service. To simulate write traffic, we wrote a small bit of code to publish RabbitMQ messages any time a write or update occurred; consumed those messages on the test user service; and duplicated the write or update. Read-and-writes were now being executed against the test service cluster without putting our production system in any real danger. Testing looked good so far. To really make sure Riak was as solid as it seemed, we then decided to ramp up the read/write load. We simulated an extreme read load by using Zeusbench, which executed an extreme number of HTTP requests against our test service and provided metrics on how the service performed (you can find a number of open source alternatives to Zeusbench). We also wrote a quick program that executed upwards of 1,500 insertions into Riak per second. We kept the inserts going the entire time. Read/write performance didn’t suffer, even under this amount of load. Our final test involved kill -9’ing Riak nodes and monitoring how the cluster performed. While removing Riak nodes, we maintained production load against the service. Riak performed well during these situations. Riak&rsquo;s strong performance suggests that, should we pursue implementation, it will withstand our unique traffic needs and prove reliable. As for the Gilt-Basho team’s strong performance: It was amazing that we were able to accomplish so much in just a week’s time! Thanks again to Seth and Steve for making this possible.","author":{"@type":"Person","name":"HBC Tech"},"@type":"BlogPosting","url":"https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2013-07-15-riak-passes-the-stress-test.html","publisher":null,"image":null,"headline":"Riak Passes the Stress Test","dateModified":"2013-07-15T13:41:00-05:00","datePublished":"2013-07-15T13:41:00-05:00","sameAs":null,"mainEntityOfPage":{"@type":"WebPage","@id":"https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2013-07-15-riak-passes-the-stress-test.html"},"@context":"http://schema.org"}</script><link rel="stylesheet" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/css/main.css"><link rel="canonical" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2013-07-15-riak-passes-the-stress-test.html"><link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,700" rel="stylesheet"><link rel="shortcut icon" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/favicon.ico"></head><body aria-label="Content"><header id="site-header" class="site-header"><div class="site-header__inner"><a class="site-header__logo" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/"><svg class="hbc-tech-logo" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 64 52"><use class="hbc-tech-logo__text" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#hbc-tech-logo"></use></svg></a><nav id="menu" class="navigation"><svg class="navigation__menu-icon" xmlns="http://www.w3.org/2000/svg"><path class="bar bar__top" d="M0,3 L30,3"></path><path class="bar bar__cross-1" d="M0,14 L30,14"></path><path class="bar bar__cross-2" d="M0,14 L30,14"></path><path class="bar bar__bottom" d="M0,25 L30,25"></path></svg><menu class="menu"><ul class="link-list"><li class="link-list__link-item"><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/" class="link-list__link"><span class="link-list__link-highlight">Insights</span></a></li><li class="link-list__link-item"><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/code" class="link-list__link"><span class="link-list__link-highlight">Code</span></a></li><li class="link-list__link-item"><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/about" class="link-list__link"><span class="link-list__link-highlight">About</span></a></li><li class="link-list__link-item"><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/work-here" class="link-list__link"><span class="link-list__link-highlight">Work Here</span></a></li></ul></menu></nav><search id="header-search" class="header-search"><form class="header-search__form"><input type="text" class="header-search__input" id="header-search-input" placeholder="Search" name="query"></form><svg id="header-search__toggle" class="header-search__svg" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 64 52"><use class="header-search__icon" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#search"></use></svg><section class="header-search__results" id="header-search__results"></section></search></div></header><section class="content"><article class="article"><header class="article__header article__header--reveal"><h1 class="header-title" title="Riak Passes the Stress Test">Riak Passes the Stress Test</h1><span id="no-image-placeholder" class="no-image-placeholder"></span> <span class="slug-divider"></span> <span class="article-meta__author"><a class="article-meta__author__link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/authors/hbc-tech">HBC Tech</a></span> <span class="slug-divider"></span> <span class="article-meta__date">JUL 15, 2013</span></header><section class="article__content article__content--reveal"><div class="article__content__read-time"><span class="read-time" title="Estimated read time"><span class="read-time__text-1">3 min</span> <span class="read-time__text-2">Read</span> <span class="read-time__text-3">Time</span></span></div><div class="article__content__share-buttons"><ul class="share-buttons"><li class="share-buttons__link-item"><a href="https://twitter.com/intent/tweet?text=https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog//2013-07-15-riak-passes-the-stress-test.html" class="share-buttons__link" title="Share on Twitter" target="_blank"><svg class="hbc-svg-icon" height="36" width="36" xmlns="http://www.w3.org/2000/svg"><use class="hbc-svg-icon--twitter" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#twitter"></use></svg></a></li><li class="share-buttons__link-item"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/&source=/2013-07-15-riak-passes-the-stress-test.html&title=Riak Passes the Stress Test" class="share-buttons__link" title="Share on Linkedin" target="_blank"><svg height="36" width="36" xmlns="http://www.w3.org/2000/svg"><use class="hbc-svg-icon" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#linkedin"></use></svg></a></li><li class="share-buttons__link-item"><a href="https://www.facebook.com/sharer/sharer.php?u=https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog//2013-07-15-riak-passes-the-stress-test.html" class="share-buttons__link" title="Share on Facebook" target="_blank"><svg height="36" width="36" xmlns="http://www.w3.org/2000/svg"><use class="hbc-svg-icon" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#facebook"></use></svg></a></li><li class="share-buttons__link-item"><a href="https://www.reddit.com/submit?url=https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog//2013-07-15-riak-passes-the-stress-test.html" class="share-buttons__link" title="Share on Reddit" target="_blank"><svg height="36" width="36" xmlns="http://www.w3.org/2000/svg"><use class="hbc-svg-icon" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#reddit"></use></svg></a></li></ul></div><div class="article__content__body"><p><img alt="image" height="198px;" id="docs-internal-guid-1aa21ddd-e340-4d53-5d6d-f2922606ab54" src="https://lh4.googleusercontent.com/wP5kUBeXSm-gO8zK7TVkAd1ovhTgf1QhMPVYJKst6B4TIfrQsm0gg9gdUZqeGmzSPeMNc7HP5HYdF-1e6rDKBit6lBHE20v7SMOXYmILwu9JccX9TBoGBBUl" width="540px;"></p><p><span>In late June a few of us in NYC joined our colleagues in Gilt&rsquo;s Dublin office to test out</span><a href="http://basho.com/riak/" target="_blank">Riak</a> for our main user store. Every day our main user store receives upwards of 100,000 requests per minute at our peak site traffic time of 12PM. Because of these extreme traffic spikes, excellent performance of our distributed databases is a must.</p><p><span>We also wanted to learn more about how Riak supported multiple data centers (including EC2) in an active-active-active configuration: a mechanism that guarantees continuous service in the event a database goes down. Active-active-active configurations also allow requests to be serviced in any datacenter without requiring the request to leave the data center to retrieve user information. These capabilities are quite important to Gilt, because network connections between datacenters are never as fast as those located within a single datacenter.</span></p><p>On the hardware side, our plan was to install two rings, each consisting of five servers, in different datacenters, and then completely run Riak through the gauntlet. We had only one week to create this system. Luckily, we had the best help imaginable: <a href="https://twitter.com/cheeseplus" target="_blank">Seth Thomas</a> and <a href="http://steve.vinoski.net/blog/" target="_blank">Steve Vinoski</a> from Basho, the company that created Riak.</p><p>Riak passed all of our tests with high marks, and also performed well during various failure scenarios we staged.</p><p>In terms of the actual testing, our goal was to duplicate our user service and point it at a Riak datastore instead of our current datastore. This, of course, required altering the service code. I&rsquo;m not a big fan of the <a href="http://www.cakesolutions.net/teamblogs/2011/12/19/cake-pattern-in-depth/" target="_blank">cake pattern</a>, which in my experience results in obfuscated code, but I have to admit that&ndash;at least in this case&ndash;it simplified the code rework and made the code cleaner.</p><p>After using Riak to duplicate our user service, our next step was to write some code to copy our existing users&rsquo; information to the new Riak backend. The speed of our current infrastructure enabled us to copy our entire user set to Riak during non-peak hours.</p><p>Once we had everything humming in our cluster of user services and Riak instance, we began simulating reads and writes to the service as they occurred in production. To do this, we used <a href="https://github.com/ebowman/splitter/blob/master/README.md" target="_blank">splitter</a>: an open source project, created by Gilt VP Architecture Eric Bowman, that accepts requests and forwards them to two different backends. One of the responses is kept, and the other is discarded. In this case we discarded the Riak response. We then configured our load balance (Zeus) to forward requests to the splitter instead of directly to the service. This put read traffic on our test user service.</p><p>To simulate write traffic, we wrote a small bit of code to publish <a href="http://www.rabbitmq.com/" target="_blank">RabbitMQ</a> messages any time a write or update occurred; consumed those messages on the test user service; and duplicated the write or update. Read-and-writes were now being executed against the test service cluster without putting our production system in any real danger. Testing looked good so far.</p><p>To really make sure Riak was as solid as it seemed, we then decided to ramp up the read/write load. We simulated an extreme read load by using <a href="http://blogs.riverbed.com/stingray/2009/03/introducing-zeusbench.html" target="_blank">Zeusbench</a>, which executed an extreme number of HTTP requests against our test service and provided metrics on how the service performed (you can find a number of open source alternatives to Zeusbench). We also wrote a quick program that executed upwards of 1,500 insertions into Riak per second. We kept the inserts going the entire time. Read/write performance didn’t suffer, even under this amount of load.</p><p>Our final test involved kill -9’ing Riak nodes and monitoring how the cluster performed. While removing Riak nodes, we maintained production load against the service. Riak performed well during these situations.</p><p>Riak&rsquo;s strong performance suggests that, should we pursue implementation, it will withstand our unique traffic needs and prove reliable. As for the Gilt-Basho team’s strong performance: It was amazing that we were able to accomplish so much in just a week’s time! Thanks again to Seth and Steve for making this possible.</p></div><footer class="article__content__footer"><div class="article-tags"><span class="article-tags__tag">splitter</span><span>,</span> <span class="article-tags__tag">zeusbench</span><span>,</span> <span class="article-tags__tag">Riak</span><span>,</span> <span class="article-tags__tag">Gilttech</span><span>,</span> <span class="article-tags__tag">Dublin</span><span>,</span> <span class="article-tags__tag">RabbitMQ</span><span>,</span> <span class="article-tags__tag">Basho</span><span>,</span> <span class="article-tags__tag">cake pattern</span><span>,</span> <span class="article-tags__tag">Jim Englert</span><span>,</span> <span class="article-tags__tag">Riakathon</span><span>,</span> <span class="article-tags__tag">gilt</span><span>,</span> <span class="article-tags__tag">Seth Thomas</span><span>,</span> <span class="article-tags__tag">Steve Vinoski</span></div><section class="author-bio"><svg class="author-bio__avatar" height="36" width="36" xmlns="http://www.w3.org/2000/svg"><use class="hbc-svg-icon hbc-svg-icon--avatar__circle" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#circle"></use><use class="hbc-svg-icon hbc-svg-icon--avatar__head" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#head"></use><use class="hbc-svg-icon hbc-svg-icon--avatar__body" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#body"></use></svg><p><a class="author-bio__link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/authors/hbc-tech">HBC Tech</a></p><p class="author-bio__bio">We power the website and mobile experiences for Saks, Saks Off Fifth, Gilt, Lord & Taylor and The Bay.</p></section></footer></section><aside class="recirc"><h2 class="header__title">Recent Insights</h2><span class="slug-divider"></span> <a class="header__view-all-link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/categories/index.html">See All</a><div class="recirc__articles"><article class="recirc__articles__item"><section class="snippet"><h1 class="snippet__title"><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2017-12-30-presentations-we-love.html" class="snippet__title__link" title="Presentations we love: 2017">Presentations we love: 2017</a></h1><div class="snippet__meta"><a class="meta__category-link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/categories/#presentations">presentations</a> <span class="slug-divider"></span> <span class="meta__author"><a class="meta__author-link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/authors/hbc-tech">HBC Tech</a></span> <span class="slug-divider"></span> <span class="meta__date">DEC 30, 2017</span></div><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2017-12-30-presentations-we-love.html" class="snippet__excerpt__link" title="Presentations we love: 2017"><p class="snippet__excerpt">2017 was a year of growth and learning at HBC Tech. Our organization embraced new technologies and new ways of building application software.</p></a></section></article><article class="recirc__articles__item"><section class="snippet"><h1 class="snippet__title"><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2017-08-10-midyear-recap.html" class="snippet__title__link" title="HBC Tech Talks: February 2017 through July 2017">HBC Tech Talks: February 2017 through July 2017</a></h1><div class="snippet__meta"><a class="meta__category-link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/category/events">events</a> <span class="slug-divider"></span> <span class="meta__author"><a class="meta__author-link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/authors/hbc-tech">HBC Tech</a></span> <span class="slug-divider"></span> <span class="meta__date">AUG 10, 2017</span></div><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2017-08-10-midyear-recap.html" class="snippet__excerpt__link" title="HBC Tech Talks: February 2017 through July 2017"><p class="snippet__excerpt">We’ve had a busy 2017 at HBC. The great work of our teams has created opportunities to share what we’ve learned with audiences around the world. This year our folks have been on stage in Austin, Sydney, Portland, Seattle, San Diego, Boston, London, Israel and on our home turf in NYC and Dublin. The talks have covered deep learning, design thinking, data streaming and developer experience to name just a...</p></a></section></article><article class="recirc__articles__item"><section class="snippet"><h1 class="snippet__title"><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2017-06-29-open-source-friday.html" class="snippet__title__link" title="Open Source Friday">Open Source Friday</a></h1><div class="snippet__meta"><a class="meta__category-link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/category/culture">culture</a> <span class="slug-divider"></span> <span class="meta__author"><a class="meta__author-link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/authors/hbc-tech">HBC Tech</a></span> <span class="slug-divider"></span> <span class="meta__date">JUN 29, 2017</span></div><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2017-06-29-open-source-friday.html" class="snippet__excerpt__link" title="Open Source Friday"><p class="snippet__excerpt">From the 54 public repos maintained at code.gilt.com to the name of our tech blog (displayed in this tab’s header), open source has been part of our team’s DNA for years. Check out this blog post from 2015 if you’re not convinced.</p></a></section></article><article class="recirc__articles__item"><section class="snippet"><h1 class="snippet__title"><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2017-06-12-hbc-at-qcon.html" class="snippet__title__link" title="Hudson's Bay Company at QCon">Hudson's Bay Company at QCon</a></h1><div class="snippet__meta"><a class="meta__category-link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/category/events">events</a> <span class="slug-divider"></span> <span class="meta__author"><a class="meta__author-link" href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/authors/hbc-tech">HBC Tech</a></span> <span class="slug-divider"></span> <span class="meta__date">JUN 12, 2017</span></div><a href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/2017-06-12-hbc-at-qcon.html" class="snippet__excerpt__link" title="Hudson's Bay Company at QCon"><p class="snippet__excerpt">Heading to QCon? Don’t miss these two sessions! If you can’t make it, stay tuned here for slides and recordings from the conference.</p></a></section></article></div></aside></article></section><footer class="footer"><svg class="est1670" xmlns="http://www.w3.org/2000/svg"><use class="hbc-tech-logo__use" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#est1670"></use></svg><section class="footer__links"><ul class="social-links social-links__list"><li class="social-links__list-item"><a class="social-links__link" target="_blank" rel="next" href="https://www.linkedin.com/company/hbc_digital/"><svg class="social-links__icon" xmlns="http://www.w3.org/2000/svg"><use class="linkedin" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#linkedin"></use></svg></a></li><li class="social-links__list-item"><a class="social-links__link" target="_blank" rel="next" href="https://twitter.com/hbcdigital/"><svg class="social-links__icon" xmlns="http://www.w3.org/2000/svg"><use class="twitter" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#twitter"></use></svg></a></li><li class="social-links__list-item"><a class="social-links__link" target="_blank" rel="next" href="https://www.instagram.com/hbcdigital"><svg class="social-links__icon" xmlns="http://www.w3.org/2000/svg"><use class="instagram" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/images/hbc-icons.svg#instagram"></use></svg></a></li></ul><p class="copyright">&copy; 2018 HBC Tech</p></section></footer><script type="text/javascript" src="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/js/vendor/scrollmagic/ScrollMagic.min.js"></script><script type="text/javascript" src="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/js/vendor/jekyll-search-js/fetch.js"></script><script type="text/javascript" src="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/js/vendor/jekyll-search-js/search.js"></script><script type="text/javascript" src="https://saksdirect.github.io/hbc-tech-blog/hbc-tech-blog/assets/js/main.js"></script></body></html>